<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Frequency-Masked Diffusion Autoencoders</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/contrib/auto-render.min.js"></script>
  <style>
    :root {
      --body-font: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Fira Sans", "Droid Sans", "Helvetica Neue", Arial, sans-serif;
      --heading-font: "Roboto Condensed", "Helvetica Neue", Helvetica, Arial, sans-serif;
      --mono-font: Menlo, monospace;
      --body-text-color: rgba(0, 0, 0, 0.8);
      --heading-text-color: rgba(0, 0, 0, 0.8);
      --paper-background: white;
      --max-width: 800px;
      --side-margin: 20px;
    }
    body {
      font-family: var(--body-font);
      color: var(--body-text-color);
      line-height: 1.6;
      background: var(--paper-background);
      margin: 0;
      padding: 0;
      font-size: 17px;
    }
    main {
      display: block;
      max-width: var(--max-width);
      margin: 0 auto;
      padding: 0 var(--side-margin);
    }
    header {
      padding: 60px 0 40px;
      text-align: center;
      max-width: 700px;
      margin: 0 auto;
    }
    h1, h2, h3 {
      font-family: var(--heading-font);
      color: var(--heading-text-color);
      font-weight: 700;
    }
    h1 { font-size: 42px; margin-bottom: 16px; }
    h2 { font-size: 26px; margin-top: 48px; margin-bottom: 16px; border-bottom: 1px solid rgba(0, 0, 0, 0.1); padding-bottom: 8px; }
    h3 { font-size: 24px; margin-top: 36px; margin-bottom: 12px; }
    .authors { margin-bottom: 32px; font-size: 17px; }
    .author { display: inline-block; margin-right: 16px; margin-bottom: 8px; }
    .abstract { font-size: 17px; line-height: 1.6; margin: 36px auto; max-width: 650px; border-left: 3px solid rgba(0, 0, 0, 0.2); padding: 0 20px; font-style: italic; color: rgba(0, 0, 0, 0.7); }
    .equation { padding: 16px; text-align: center; font-size: 18px; }
    .references .citation { color: rgba(0, 0, 0, 0.6); font-size: 15px; margin-bottom: 12px; }
    .references { margin-bottom: 64px; }
    .toc a { text-decoration: none; color: rgba(0, 0, 0, 0.7); }
    .toc a:hover { text-decoration: underline; color: #0366d6; }
    .sticky-toc {
      position: fixed;
      width: 180px;
      top: 20px;
      left: max(calc(50% - 600px), 20px);
    }
  </style>
</head>

<body>
  <div class="sticky-toc">
    <h3>Contents</h3>
    <nav class="toc">
      <ul>
        <li><a href="#introduction">Introduction</a></li>
        <li><a href="#related-work">Related Work</a></li>
        <li><a href="#methods">Proposed Methodology</a></li>
        <li><a href="#references">References</a></li>
      </ul>
    </nav>
  </div>

  <header>
    <h1>Frequency-Masked Diffusion Autoencoders for Multiscale Representation Learning</h1>
    <div class="authors">
      <div class="author">Mason Wang</div>
      <div class="author">Stephen Brade</div>
    </div>
    <div class="date">Final project for 6.7960, MIT</div>
  </header>

  <main>
    <div class="d-article">
      <div class="d-article-body">
        <h2 id="introduction">Introduction</h2>
        <p>The Fourier transform is an orthogonal basis transformation that correlates the input signal with sinusoids at multiple frequencies. Applied to images, the Fourier transform provides a frequency domain representation of the image, capturing pixel-wise spatial variations occurring at different frequencies. This representation has several properties that we desire in a multiscale image representation: it is invertible, easily interpretable, and different <code>scales</code> (frequencies) are independent of one another. Also, we can flexibly aggregate information from multiple scales by designing filters with a specified frequency response.</p>
        <p>Despite its elegant mathematical properties, the Fourier representation often lacks the level of semantic meaning we desire from an ideal multiscale representation. For instance, we might want the largest scale to aggregate information across the entire image and produce a vector that captures its global characteristics. However, the Fourier transform's largest scale only captures the average pixel value of the image. Similarly, selecting only the <code>large-scale</code> (low-frequency) features of an image's Fourier transform is equivalent to blurring the image, while we might want it to capture large-scale <em>semantic</em> variations in the image instead.</p>
        <p>Deep learning provides an alternative approach to multi-scale representation learning. The hierarchical structure of CNNs often results in a type of scale-based organization, where layers at a higher resolution correspond to small-scale features, and layers at a lower resolution correspond to large-scale features [1]. Multi-scale VAEs and vector-quantization techniques have also been used to achieve deep, multi-scale representations [2, 3, 4]. While these representations can be semantically meaningful, they have several drawbacks that are not present in the frequency domain representation.</p>
        <p>First, the <code>scale</code> parameter is heavily discretized. The number of unique scales is often associated with the number of layers or quantization levels. Second, different scales are not independent. Small-scale features are often obtained by feeding forward large-scale features, or vice-versa. Third, it is difficult to aggregate information across different scales. Representations at different scales often come in different shapes, and the lack of orthogonality between scales can result in unexpected behavior.</p>
        <p>In essence, we seek a multi-scale representation of images that combines the semantic power of deep-learning-based approaches with the continuity and orthogonality of the Fourier transform. To this end, we propose a novel technique for learning multi-scale features from an image, based on an extension of diffusion autoencoders.</p>

        <h2 id="related-work">Related Work</h2>
        <p><strong>Diffusion Models.</strong> Diffusion models have driven major advances in high-fidelity image generation and representation learning. DDPMs [11] laid the foundation for these models, later extended to high-resolution, text-conditional synthesis [12], and refined through architectural improvements and training strategies [13, 14]. Controllability has become increasingly important. ControlNet [15] introduces spatially aligned conditions (e.g., depth, pose) into the diffusion process, while Uni-ControlNet [16] generalizes this across modalities without retraining the base model. Diffusion autoencoders [5] encode images into non-spatial latents \(\mathbf{z}\) that condition a diffusion model for reconstruction. These latents are semantically meaningful but lack structure across spatial scales, unlike the Fourier transform which separates frequencies cleanly. We propose a frequency-masked diffusion autoencoder that conditions generation on specific frequency bands.</p>

        <p><strong>Masked Autoencoders.</strong> Inspired by masked language modeling in NLP, masked autoencoders (MAEs) have become a powerful approach for unsupervised visual representation learning. BEiT [17] introduces a two-stage process involving token prediction of masked image patches. MAE [18] simplifies this by predicting raw pixels directly, showing strong performance with end-to-end learning. SimMIM [19] confirms the effectiveness of this pixel-level reconstruction strategy. Other works refine masking strategies [20], introduce uniform masking in hierarchical architectures [21], and explore CNN-transformer hybrids [22]. While existing MAEs focus on spatial masking, our method introduces frequency-masked reconstruction to produce scale-aware representations and enable controllable manipulation of image structure.</p>

        <p><strong>Multi-Scale Representation Learning in Generative Models.</strong> Multiscale representation of images are becoming increasingly important for unsupervised learning. In image generation, a very common theme is to generate the large-scale characteristics of an image, followed by small-scale characteristics. While pixel diffusion models implicitly do this during the reverse process [6], latent [7] and multi-stage [8] diffusion models explicitly predict images or features at lower resolutions before upsampling them. Recent work also frames <code>next-scale prediction</code> as a sequence modeling problem [4]. These generative modeling techniques each contain an implicit or explicit <code>multi-scale</code> representation. In contrast, our approach models frequency bands directly, enabling multiscale control without requiring spatial downsampling or hierarchical generation.</p>
        
        <h2 id=\"methods\">Proposed Methodology</h2>
        <p>Our proposed method mirrors the training strategy of a standard diffusion autoencoder. However, instead of producing a single non-spatial latent, the encoder outputs a latent feature map. This map is then filtered through a randomly selected frequency bandpass before being passed to the diffusion model alongside a noisy image. At test time, we can perform conditional generation using any filtered latent—such as only the low or high frequency content—or a composition of multiple latents from different images, similar to SEGA [10].</p>
        <figure style="margin-top: 20px; margin-bottom: 20px;">
        <img src="images/diagrampng.png" style="width: 100%; max-width: 100%; height: auto; display: block; margin: 0 auto;">
        <figcaption style="text-align: center; font-size: 16px; margin-top: 8px;">
            Figure 1: Diagram of our proposed training method. The encoder produces a latent feature map that is randomly bandpassed before being input to the diffusion model.
        </figcaption>
        </figure>

        <h2 id="references">References</h2>
        <div class="references">
          <div class="citation">[1] Zeiler & Fergus, 2014. Visualizing and understanding convolutional networks. ECCV.</div>
          <div class="citation">[2] Gulrajani et al., 2016. PixelVAE. arXiv:1611.05013.</div>
          <div class="citation">[3] Vahdat & Kautz, 2020. NVAE. NeurIPS.</div>
          <div class="citation">[4] Tian et al., 2024. Visual autoregressive modeling. NeurIPS.</div>
          <div class="citation">[5] Preechakul et al., 2022. Diffusion autoencoders. CVPR.</div>
          <div class="citation">[6] Dieleman, 2024. Diffusion is spectral autoregression.</div>
          <div class="citation">[7] Rombach et al., 2022. High-resolution latent diffusion. CVPR.</div>
          <div class="citation">[8] Ho et al., 2022. Cascaded diffusion models. JMLR.</div>
          <div class="citation">[10] Brack et al., 2023. SEGA: Semantic Guidance. NeurIPS.</div>
          <div class="citation">[11] Ho et al., 2020. Denoising diffusion probabilistic models. NeurIPS.</div>
          <div class="citation">[12] Saharia et al., 2022. Photorealistic text-to-image diffusion. NeurIPS.</div>
          <div class="citation">[13] Karras et al., 2022. Elucidating diffusion design. NeurIPS.</div>
          <div class="citation">[14] Kwon et al., 2022. Semantic latent space in diffusion. arXiv.</div>
          <div class="citation">[15] Zhang et al., 2023. Adding conditional control to diffusion. arXiv.</div>
          <div class="citation">[16] Zhao et al., 2023. Uni-ControlNet. NeurIPS.</div>
        <div class="citation">[17] Bao et al., 2022. BEiT: BERT Pre-Training of Image Transformers. ICLR.</div>
        <div class="citation">[18] He et al., 2022. Masked Autoencoders Are Scalable Vision Learners. CVPR.</div>
        <div class="citation">[19] Xie et al., 2022. SimMIM: A Simple Framework for Masked Image Modeling. CVPR.</div>
        <div class="citation">[20] Chen et al., 2022. Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. arXiv.</div>
        <div class="citation">[21] Li et al., 2022. Uniform Masking for Pyramid-based Vision Transformers. arXiv.</div>
        <div class="citation">[22] Gao et al., 2022. ConvMAE: Masked Convolution Meets Masked Autoencoders. arXiv.</div>
        </div>
      </div>
    </div>
  </main>

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      if (typeof renderMathInElement !== 'undefined') {
        renderMathInElement(document.body, {
          delimiters: [
            { left: "\\[", right: "\\]", display: true },
            { left: "\\(", right: "\\)", display: false },
            { left: "$", right: "$", display: false },
            { left: "$$", right: "$$", display: true }
          ],
          throwOnError: false
        });
      }
    });
  </script>
</body>
</html>
